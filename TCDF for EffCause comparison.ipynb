{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T03:14:46.630952Z",
     "start_time": "2021-10-11T03:14:46.584045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import TCDF\n",
    "import argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pylab\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set CUDA gpu device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.chdir(os.path.dirname(sys.argv[0])) #uncomment this line to run in VSCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T03:07:00.624581Z",
     "start_time": "2021-10-11T03:07:00.525734Z"
    }
   },
   "outputs": [],
   "source": [
    "def getextendeddelays(gtfile, columns):\n",
    "    \"\"\"Collects the total delay of indirect causal relationships.\"\"\"\n",
    "#     print(columns)\n",
    "    gtdata = pd.read_csv(gtfile, header=None)\n",
    "\n",
    "    readgt=dict()\n",
    "    effects = gtdata[1]\n",
    "    causes = gtdata[0]\n",
    "    delays = gtdata[2]\n",
    "    gtnrrelations = 0\n",
    "    pairdelays = dict()\n",
    "    for k in range(len(columns)):\n",
    "        readgt[k]=[]\n",
    "    for i in range(len(effects)):\n",
    "        key=effects[i]\n",
    "        value=causes[i]\n",
    "        readgt[key].append(value)\n",
    "        pairdelays[(key, value)]=delays[i]\n",
    "        gtnrrelations+=1\n",
    "    \n",
    "    g = nx.DiGraph()\n",
    "    g.add_nodes_from(readgt.keys())\n",
    "    for e in readgt:\n",
    "        cs = readgt[e]\n",
    "        for c in cs:\n",
    "            g.add_edge(c, e)\n",
    "\n",
    "    extendedreadgt = copy.deepcopy(readgt)\n",
    "    \n",
    "    for c1 in range(len(columns)):\n",
    "        for c2 in range(len(columns)):\n",
    "            paths = list(nx.all_simple_paths(g, c1, c2, cutoff=2)) #indirect path max length 3, no cycles\n",
    "            \n",
    "            if len(paths)>0:\n",
    "                for path in paths:\n",
    "                    for p in path[:-1]:\n",
    "                        if p not in extendedreadgt[path[-1]]:\n",
    "                            extendedreadgt[path[-1]].append(p)\n",
    "                            \n",
    "    extendedgtdelays = dict()\n",
    "    for effect in extendedreadgt:\n",
    "        causes = extendedreadgt[effect]\n",
    "        for cause in causes:\n",
    "            if (effect, cause) in pairdelays:\n",
    "                delay = pairdelays[(effect, cause)]\n",
    "                extendedgtdelays[(effect, cause)]=[delay]\n",
    "            else:\n",
    "                #find extended delay\n",
    "                paths = list(nx.all_simple_paths(g, cause, effect, cutoff=2)) #indirect path max length 3, no cycles\n",
    "                extendedgtdelays[(effect, cause)]=[]\n",
    "                for p in paths:\n",
    "                    delay=0\n",
    "                    for i in range(len(p)-1):\n",
    "                        delay+=pairdelays[(p[i+1], p[i])]\n",
    "                    extendedgtdelays[(effect, cause)].append(delay)\n",
    "\n",
    "    return extendedgtdelays, readgt, extendedreadgt\n",
    "\n",
    "def evaluate(gtfile, validatedcauses, columns, verbose=True):\n",
    "    \"\"\"Evaluates the results of TCDF by comparing it to the ground truth graph, and calculating precision, recall and F1-score. F1'-score, precision' and recall' include indirect causal relationships.\"\"\"\n",
    "    extendedgtdelays, readgt, extendedreadgt = getextendeddelays(gtfile, columns)\n",
    "    FP=0\n",
    "    FPdirect=0\n",
    "    TPdirect=0\n",
    "    TP=0\n",
    "    FN=0\n",
    "    FPs = []\n",
    "    FPsdirect = []\n",
    "    TPsdirect = []\n",
    "    TPs = []\n",
    "    FNs = []\n",
    "    for key in readgt:\n",
    "        for v in validatedcauses[key]:\n",
    "            if v not in extendedreadgt[key]:\n",
    "                FP+=1\n",
    "                FPs.append((key,v))\n",
    "            else:\n",
    "                TP+=1\n",
    "                TPs.append((key,v))\n",
    "            if v not in readgt[key]:\n",
    "                FPdirect+=1\n",
    "                FPsdirect.append((key,v))\n",
    "            else:\n",
    "                TPdirect+=1\n",
    "                TPsdirect.append((key,v))\n",
    "        for v in readgt[key]:\n",
    "            if v not in validatedcauses[key]:\n",
    "                FN+=1\n",
    "                FNs.append((key, v))\n",
    "        \n",
    "    \n",
    "    precision = recall = 0.\n",
    "\n",
    "    if float(TP+FP)>0:\n",
    "        precision = TP / float(TP+FP)\n",
    "    \n",
    "    if float(TP + FN)>0:\n",
    "        recall = TP / float(TP + FN)\n",
    "    \n",
    "    if (precision + recall) > 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0.\n",
    "\n",
    "    precision = recall = 0.\n",
    "    if float(TPdirect+FPdirect)>0:\n",
    "        precision = TPdirect / float(TPdirect+FPdirect)\n",
    "    \n",
    "    if float(TPdirect + FN)>0:\n",
    "        recall = TPdirect / float(TPdirect + FN)\n",
    "    \n",
    "    if (precision + recall) > 0:\n",
    "        F1direct = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1direct = 0.\n",
    "    if verbose:\n",
    "        print(\"Total False Positives': \", FP)\n",
    "        print(\"Total True Positives': \", TP)\n",
    "        print(\"Total False Negatives: \", FN)\n",
    "        print(\"Total Direct False Positives: \", FPdirect)\n",
    "        print(\"Total Direct True Positives: \", TPdirect)\n",
    "        print(\"TPs': \", TPs)\n",
    "        print(\"FPs': \", FPs)\n",
    "        print(\"TPs direct: \", TPsdirect)\n",
    "        print(\"FPs direct: \", FPsdirect)\n",
    "        print(\"FNs: \", FNs)\n",
    "        print(\"Precision': \", precision)\n",
    "        print(\"Recall': \", recall)\n",
    "        print(\"F1' score: \", F1,\"(includes direct and indirect causal relationships)\")\n",
    "        print(\"Precision: \", precision)\n",
    "        print(\"Recall: \", recall)\n",
    "        print(\"F1 score: \", F1direct,\"(includes only direct causal relationships)\")\n",
    "    return FP, TP, FPdirect, TPdirect, FN, FPs, FPsdirect, TPs, TPsdirect, FNs, F1, F1direct\n",
    "\n",
    "def evaluatedelay(extendedgtdelays, alldelays, TPs, receptivefield):\n",
    "    \"\"\"Evaluates the delay discovery of TCDF by comparing the discovered time delays with the ground truth.\"\"\"\n",
    "    zeros = 0\n",
    "    total = 0.\n",
    "    for i in range(len(TPs)):\n",
    "        tp=TPs[i]\n",
    "        discovereddelay = alldelays[tp]\n",
    "        gtdelays = extendedgtdelays[tp]\n",
    "        for d in gtdelays:\n",
    "            if d <= receptivefield:\n",
    "                total+=1.\n",
    "                error = d - discovereddelay\n",
    "                if error == 0:\n",
    "                    zeros+=1\n",
    "                \n",
    "            else:\n",
    "                next\n",
    "           \n",
    "    if zeros==0:\n",
    "        return 0.\n",
    "    else:\n",
    "        return zeros/float(total)\n",
    "    \n",
    "    \n",
    "def runTCDF(datafile, args):\n",
    "    \"\"\"Loops through all variables in a dataset and return the discovered causes, time delays, losses, attention scores and variable names.\"\"\"\n",
    "    df_data = pd.read_csv(datafile, header=0)\n",
    "    allcauses = dict()\n",
    "    alldelays = dict()\n",
    "    allreallosses=dict()\n",
    "    allscores=dict()\n",
    "\n",
    "    columns = list(df_data)\n",
    "    for c in columns:\n",
    "        idx = df_data.columns.get_loc(c)\n",
    "        causes, causeswithdelay, realloss, scores = TCDF.findcauses(c, cuda=args.cuda, epochs=args.epochs, \n",
    "            kernel_size=args.kernel_size, layers=args.hidden_layers+1, log_interval=args.log_interval, \n",
    "            lr=args.learning_rate, optimizername=args.optimizer,\n",
    "            seed=args.seed, dilation_c=args.dilation_coefficient, \n",
    "            significance=args.significance, file=datafile, verbose=args.verbose)\n",
    "\n",
    "        allscores[idx]=scores\n",
    "        allcauses[idx]=causes\n",
    "        alldelays.update(causeswithdelay)\n",
    "        allreallosses[idx]=realloss\n",
    "\n",
    "    return allcauses, alldelays, allreallosses, allscores, columns\n",
    "\n",
    "def plotgraph(stringdatafile,alldelays,columns):\n",
    "    \"\"\"Plots a temporal causal graph showing all discovered causal relationships annotated with the time delay between cause and effect.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    for c in columns:\n",
    "        G.add_node(c)\n",
    "    for pair in alldelays:\n",
    "        p1,p2 = pair\n",
    "        nodepair = (columns[p2], columns[p1])\n",
    "\n",
    "        G.add_edges_from([nodepair],weight=alldelays[pair])\n",
    "    \n",
    "    edge_labels=dict([((u,v,),d['weight'])\n",
    "                    for u,v,d in G.edges(data=True)])\n",
    "    \n",
    "    pos=nx.circular_layout(G)\n",
    "    nx.draw_networkx_edge_labels(G,pos,edge_labels=edge_labels)\n",
    "    nx.draw(G,pos, node_color = 'white', edge_color='black',node_size=1000,with_labels = True)\n",
    "    ax = plt.gca()\n",
    "    ax.collections[0].set_edgecolor(\"#000000\") \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def main(datafiles, args, evaluation):\n",
    "    if evaluation:\n",
    "        totalF1direct = [] #contains F1-scores of all datasets\n",
    "        totalF1 = [] #contains F1'-scores of all datasets\n",
    "\n",
    "        receptivefield=1\n",
    "        for l in range(0, levels):\n",
    "            receptivefield+=(kernel_size-1) * dilation_c**(l)\n",
    "\n",
    "    exp_records = []\n",
    "    for datafile in datafiles.keys(): \n",
    "        stringdatafile = str(datafile)\n",
    "        if '/' in stringdatafile:\n",
    "            stringdatafile = str(datafile).rsplit('/', 1)[1]\n",
    "        if args.verbose:\n",
    "            print(\"\\n Dataset: \", stringdatafile)\n",
    "\n",
    "        # run TCDF\n",
    "        allcauses, alldelays, allreallosses, allscores, columns = runTCDF(datafile, args) #results of TCDF containing indices of causes and effects\n",
    "        d = {\n",
    "            \"allcauses\": allcauses,\n",
    "            \"alldelays\": alldelays,\n",
    "            \"allreallosses\": allreallosses,\n",
    "            \"allscores\": allscores,\n",
    "            \"columns\": columns\n",
    "        }\n",
    "        if args.verbose:\n",
    "            print(\"\\n===================Results for\", stringdatafile,\"==================================\")\n",
    "            for pair in alldelays:\n",
    "                print(columns[pair[1]], \"causes\", columns[pair[0]],\"with a delay of\",alldelays[pair],\"time steps.\")\n",
    "\n",
    "        \n",
    "\n",
    "        if evaluation:\n",
    "            # evaluate TCDF by comparing discovered causes with ground truth\n",
    "            if args.verbose:\n",
    "                print(\"\\n===================Evaluation for\", stringdatafile,\"===============================\")\n",
    "            FP, TP, FPdirect, TPdirect, FN, FPs, FPsdirect, TPs, TPsdirect, FNs, F1, F1direct \\\n",
    "                = evaluate(datafiles[datafile], allcauses, columns, verbose=args.verbose)\n",
    "            # Update to records\n",
    "            for k, v in zip(\n",
    "                [\"FP\", \"TP\", \"FPdirect\", \"TPdirect\", \"FN\", \"FPs\", \"FPsdirect\", \n",
    "                 \"TPs\", \"TPsdirect\", \"FNs\", \"F1\", \"F1direct\"],\n",
    "                [FP, TP, FPdirect, TPdirect, FN, FPs, FPsdirect, TPs, TPsdirect, FNs, F1, F1direct]\n",
    "            ):\n",
    "                d[k] = v\n",
    "            totalF1.append(F1)\n",
    "            totalF1direct.append(F1direct)\n",
    "\n",
    "            # evaluate delay discovery\n",
    "            extendeddelays, readgt, extendedreadgt = getextendeddelays(datafiles[datafile], columns)\n",
    "            percentagecorrect = evaluatedelay(extendeddelays, alldelays, TPs, receptivefield)*100\n",
    "            if args.verbose:\n",
    "                print(\"Percentage of delays that are correctly discovered: \", percentagecorrect,\"%\")\n",
    "        if args.verbose:\n",
    "            print(\"==================================================================================\")\n",
    "        # Save to records\n",
    "        exp_records.append(d)\n",
    "        if args.plot:\n",
    "            plotgraph(stringdatafile, alldelays, columns)\n",
    "\n",
    "    # In case of multiple datasets, calculate average F1-score over all datasets and standard deviation\n",
    "    if len(datafiles.keys())>1 and evaluation and args.verbose:  \n",
    "        print(\"\\nOverall Evaluation: \\n\")      \n",
    "        print(\"F1' scores: \")\n",
    "        for f in totalF1:\n",
    "            print(f)\n",
    "        print(\"Average F1': \", np.mean(totalF1))\n",
    "        print(\"Standard Deviation F1': \", np.std(totalF1),\"\\n\")\n",
    "        print(\"F1 scores: \")\n",
    "        for f in totalF1direct:\n",
    "            print(f)\n",
    "        print(\"Average F1: \", np.mean(totalF1direct))\n",
    "        print(\"Standard Deviation F1: \", np.std(totalF1direct))\n",
    "    return exp_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T02:04:08.784931Z",
     "start_time": "2021-08-16T02:04:08.524239Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Finance/manyinputs_returns30007000_header.csv=data/Finance/manyinputs.csv,data/Finance/random-rels_20_1A_returns30007000_header.csv=data/Finance/random-rels_20_1A.csv,data/Finance/random-rels_20_1B_returns30007000_header.csv=data/Finance/random-rels_20_1B.csv,data/Finance/random-rels_20_1C_returns30007000_header.csv=data/Finance/random-rels_20_1C.csv,data/Finance/random-rels_20_1D_returns30007000_header.csv=data/Finance/random-rels_20_1D.csv,data/Finance/random-rels_20_1E_returns30007000_header.csv=data/Finance/random-rels_20_1E.csv,data/Finance/random-rels_40_1_returns30007000_header.csv=data/Finance/random-rels_20_1_3.csv,data/Finance/random-rels_40_1_3_returns30007000_header.csv=data/Finance/random-rels_40_1.csv,data/Finance/random-rels_20_1_3_returns30007000_header.csv=data/Finance/random-rels_40_1_3.csv\n"
     ]
    }
   ],
   "source": [
    "tcdf_data_path = 'data/Finance/'\n",
    "timeseries_files = ['manyinputs_returns30007000_header.csv',\n",
    "                    'random-rels_20_1A_returns30007000_header.csv',\n",
    "                    'random-rels_20_1B_returns30007000_header.csv',\n",
    "                    'random-rels_20_1C_returns30007000_header.csv',\n",
    "                    'random-rels_20_1D_returns30007000_header.csv',\n",
    "                    'random-rels_20_1E_returns30007000_header.csv',\n",
    "                    'random-rels_40_1_returns30007000_header.csv',\n",
    "                    'random-rels_40_1_3_returns30007000_header.csv',\n",
    "                    'random-rels_20_1_3_returns30007000_header.csv'\n",
    "                   ]\n",
    "gt_files = ['manyinputs.csv',\n",
    "            'random-rels_20_1A.csv',\n",
    "            'random-rels_20_1B.csv',\n",
    "            'random-rels_20_1C.csv',\n",
    "            'random-rels_20_1D.csv',\n",
    "            'random-rels_20_1E.csv',\n",
    "            'random-rels_20_1_3.csv',\n",
    "            'random-rels_40_1.csv',\n",
    "            'random-rels_40_1_3.csv'\n",
    "           ]\n",
    "s = \"\"\n",
    "for i in range(len(timeseries_files)):\n",
    "    s += tcdf_data_path+timeseries_files[i]+\"=\"+tcdf_data_path+gt_files[i]\n",
    "    if i != len(timeseries_files)-1:\n",
    "        s += \",\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T03:13:58.653575Z",
     "start_time": "2021-10-11T03:13:58.520613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/fMRI/timeseries5.csv=data/fMRI/sim5_gt_processed.csv,data/fMRI/timeseries6.csv=data/fMRI/sim6_gt_processed.csv,data/fMRI/timeseries7.csv=data/fMRI/sim7_gt_processed.csv,data/fMRI/timeseries9.csv=data/fMRI/sim9_gt_processed.csv,data/fMRI/timeseries19.csv=data/fMRI/sim19_gt_processed.csv,data/fMRI/timeseries20.csv=data/fMRI/sim20_gt_processed.csv\n"
     ]
    }
   ],
   "source": [
    "tcdf_data_path = 'data/fMRI/'\n",
    "s = \"\"\n",
    "for i in range(1, 29, 1):\n",
    "    data = pd.read_csv(tcdf_data_path + f\"timeseries{i}.csv\", header=0)\n",
    "    if data.shape[0] > 1000:\n",
    "        s += tcdf_data_path + f\"timeseries{i}.csv\" + \"=\" + tcdf_data_path + f\"sim{i}_gt_processed.csv\"\n",
    "        s += \",\"\n",
    "s = s[:-1]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T03:08:28.012682Z",
     "start_time": "2021-10-11T03:08:27.975276Z"
    }
   },
   "outputs": [],
   "source": [
    "s='data/Finance/manyinputs_returns30007000_header.csv=data/Finance/manyinputs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T03:14:03.362811Z",
     "start_time": "2021-10-11T03:14:03.320236Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: Namespace(cuda=True, data=None, dilation_coefficient=4, epochs=500, ground_truth={'data/fMRI/timeseries5.csv': 'data/fMRI/sim5_gt_processed.csv', 'data/fMRI/timeseries6.csv': 'data/fMRI/sim6_gt_processed.csv', 'data/fMRI/timeseries7.csv': 'data/fMRI/sim7_gt_processed.csv', 'data/fMRI/timeseries9.csv': 'data/fMRI/sim9_gt_processed.csv', 'data/fMRI/timeseries19.csv': 'data/fMRI/sim19_gt_processed.csv', 'data/fMRI/timeseries20.csv': 'data/fMRI/sim20_gt_processed.csv'}, hidden_layers=3, kernel_size=4, learning_rate=0.01, log_interval=100, optimizer='Adam', plot=False, seed=1111, significance=0.8, train_test_split=0.8, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from args import build_argparser\n",
    "parser = build_argparser()\n",
    "args = parser.parse_args(['--ground_truth', s, \n",
    "#                           '--plot', \n",
    "                          '--hidden_layers', '3', \n",
    "                          '--log_interval', '100',\n",
    "                          '--learning_rate', '0.01',\n",
    "                          '--epochs', '500',\n",
    "#                           '--verbose',\n",
    "                          '--cuda'])\n",
    "print(\"Arguments:\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-11T03:55:43.383863Z",
     "start_time": "2021-10-11T03:40:20.483160Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: Namespace(cuda=True, data=None, dilation_coefficient=4, epochs=500, ground_truth={'data/fMRI/timeseries5.csv': 'data/fMRI/sim5_gt_processed.csv', 'data/fMRI/timeseries6.csv': 'data/fMRI/sim6_gt_processed.csv', 'data/fMRI/timeseries7.csv': 'data/fMRI/sim7_gt_processed.csv', 'data/fMRI/timeseries9.csv': 'data/fMRI/sim9_gt_processed.csv', 'data/fMRI/timeseries19.csv': 'data/fMRI/sim19_gt_processed.csv', 'data/fMRI/timeseries20.csv': 'data/fMRI/sim20_gt_processed.csv'}, hidden_layers=3, kernel_size=4, learning_rate=0.01, log_interval=100, optimizer='Adam', plot=False, seed=1111, significance=0.2, train_test_split=0.8, verbose=False)\n",
      "/workspace/dycause-efficiency-analysis/temp_results/tcdf/tcdf_fmri/exp_records_sign0.2_20211011_114215.pkl\n",
      "Arguments: Namespace(cuda=True, data=None, dilation_coefficient=4, epochs=500, ground_truth={'data/fMRI/timeseries5.csv': 'data/fMRI/sim5_gt_processed.csv', 'data/fMRI/timeseries6.csv': 'data/fMRI/sim6_gt_processed.csv', 'data/fMRI/timeseries7.csv': 'data/fMRI/sim7_gt_processed.csv', 'data/fMRI/timeseries9.csv': 'data/fMRI/sim9_gt_processed.csv', 'data/fMRI/timeseries19.csv': 'data/fMRI/sim19_gt_processed.csv', 'data/fMRI/timeseries20.csv': 'data/fMRI/sim20_gt_processed.csv'}, hidden_layers=3, kernel_size=4, learning_rate=0.01, log_interval=100, optimizer='Adam', plot=False, seed=1111, significance=0.30000000000000004, train_test_split=0.8, verbose=False)\n",
      "/workspace/dycause-efficiency-analysis/temp_results/tcdf/tcdf_fmri/exp_records_sign0.30000000000000004_20211011_114409.pkl\n",
      "Arguments: Namespace(cuda=True, data=None, dilation_coefficient=4, epochs=500, ground_truth={'data/fMRI/timeseries5.csv': 'data/fMRI/sim5_gt_processed.csv', 'data/fMRI/timeseries6.csv': 'data/fMRI/sim6_gt_processed.csv', 'data/fMRI/timeseries7.csv': 'data/fMRI/sim7_gt_processed.csv', 'data/fMRI/timeseries9.csv': 'data/fMRI/sim9_gt_processed.csv', 'data/fMRI/timeseries19.csv': 'data/fMRI/sim19_gt_processed.csv', 'data/fMRI/timeseries20.csv': 'data/fMRI/sim20_gt_processed.csv'}, hidden_layers=3, kernel_size=4, learning_rate=0.01, log_interval=100, optimizer='Adam', plot=False, seed=1111, significance=0.4, train_test_split=0.8, verbose=False)\n",
      "/workspace/dycause-efficiency-analysis/temp_results/tcdf/tcdf_fmri/exp_records_sign0.4_20211011_114603.pkl\n",
      "Arguments: Namespace(cuda=True, data=None, dilation_coefficient=4, epochs=500, ground_truth={'data/fMRI/timeseries5.csv': 'data/fMRI/sim5_gt_processed.csv', 'data/fMRI/timeseries6.csv': 'data/fMRI/sim6_gt_processed.csv', 'data/fMRI/timeseries7.csv': 'data/fMRI/sim7_gt_processed.csv', 'data/fMRI/timeseries9.csv': 'data/fMRI/sim9_gt_processed.csv', 'data/fMRI/timeseries19.csv': 'data/fMRI/sim19_gt_processed.csv', 'data/fMRI/timeseries20.csv': 'data/fMRI/sim20_gt_processed.csv'}, hidden_layers=3, kernel_size=4, learning_rate=0.01, log_interval=100, optimizer='Adam', plot=False, seed=1111, significance=0.5, train_test_split=0.8, verbose=False)\n",
      "/workspace/dycause-efficiency-analysis/temp_results/tcdf/tcdf_fmri/exp_records_sign0.5_20211011_114757.pkl\n",
      "Arguments: Namespace(cuda=True, data=None, dilation_coefficient=4, epochs=500, ground_truth={'data/fMRI/timeseries5.csv': 'data/fMRI/sim5_gt_processed.csv', 'data/fMRI/timeseries6.csv': 'data/fMRI/sim6_gt_processed.csv', 'data/fMRI/timeseries7.csv': 'data/fMRI/sim7_gt_processed.csv', 'data/fMRI/timeseries9.csv': 'data/fMRI/sim9_gt_processed.csv', 'data/fMRI/timeseries19.csv': 'data/fMRI/sim19_gt_processed.csv', 'data/fMRI/timeseries20.csv': 'data/fMRI/sim20_gt_processed.csv'}, hidden_layers=3, kernel_size=4, learning_rate=0.01, log_interval=100, optimizer='Adam', plot=False, seed=1111, significance=0.6, train_test_split=0.8, verbose=False)\n",
      "/workspace/dycause-efficiency-analysis/temp_results/tcdf/tcdf_fmri/exp_records_sign0.6_20211011_114951.pkl\n",
      "Arguments: Namespace(cuda=True, data=None, dilation_coefficient=4, epochs=500, ground_truth={'data/fMRI/timeseries5.csv': 'data/fMRI/sim5_gt_processed.csv', 'data/fMRI/timeseries6.csv': 'data/fMRI/sim6_gt_processed.csv', 'data/fMRI/timeseries7.csv': 'data/fMRI/sim7_gt_processed.csv', 'data/fMRI/timeseries9.csv': 'data/fMRI/sim9_gt_processed.csv', 'data/fMRI/timeseries19.csv': 'data/fMRI/sim19_gt_processed.csv', 'data/fMRI/timeseries20.csv': 'data/fMRI/sim20_gt_processed.csv'}, hidden_layers=3, kernel_size=4, learning_rate=0.01, log_interval=100, optimizer='Adam', plot=False, seed=1111, significance=0.7000000000000001, train_test_split=0.8, verbose=False)\n",
      "/workspace/dycause-efficiency-analysis/temp_results/tcdf/tcdf_fmri/exp_records_sign0.7000000000000001_20211011_115146.pkl\n",
      "Arguments: Namespace(cuda=True, data=None, dilation_coefficient=4, epochs=500, ground_truth={'data/fMRI/timeseries5.csv': 'data/fMRI/sim5_gt_processed.csv', 'data/fMRI/timeseries6.csv': 'data/fMRI/sim6_gt_processed.csv', 'data/fMRI/timeseries7.csv': 'data/fMRI/sim7_gt_processed.csv', 'data/fMRI/timeseries9.csv': 'data/fMRI/sim9_gt_processed.csv', 'data/fMRI/timeseries19.csv': 'data/fMRI/sim19_gt_processed.csv', 'data/fMRI/timeseries20.csv': 'data/fMRI/sim20_gt_processed.csv'}, hidden_layers=3, kernel_size=4, learning_rate=0.01, log_interval=100, optimizer='Adam', plot=False, seed=1111, significance=0.8, train_test_split=0.8, verbose=False)\n",
      "/workspace/dycause-efficiency-analysis/temp_results/tcdf/tcdf_fmri/exp_records_sign0.8_20211011_115342.pkl\n",
      "Arguments: Namespace(cuda=True, data=None, dilation_coefficient=4, epochs=500, ground_truth={'data/fMRI/timeseries5.csv': 'data/fMRI/sim5_gt_processed.csv', 'data/fMRI/timeseries6.csv': 'data/fMRI/sim6_gt_processed.csv', 'data/fMRI/timeseries7.csv': 'data/fMRI/sim7_gt_processed.csv', 'data/fMRI/timeseries9.csv': 'data/fMRI/sim9_gt_processed.csv', 'data/fMRI/timeseries19.csv': 'data/fMRI/sim19_gt_processed.csv', 'data/fMRI/timeseries20.csv': 'data/fMRI/sim20_gt_processed.csv'}, hidden_layers=3, kernel_size=4, learning_rate=0.01, log_interval=100, optimizer='Adam', plot=False, seed=1111, significance=0.9, train_test_split=0.8, verbose=False)\n",
      "/workspace/dycause-efficiency-analysis/temp_results/tcdf/tcdf_fmri/exp_records_sign0.9_20211011_115543.pkl\n"
     ]
    }
   ],
   "source": [
    "for sign in np.arange(0.1, 1.0, 0.1):\n",
    "    if sign==0.1:\n",
    "        continue\n",
    "    args.significance = sign\n",
    "    print(\"Arguments:\", args)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        if not args.cuda:\n",
    "            print(\"WARNING: You have a CUDA device, you should probably run with --cuda to speed up training.\")\n",
    "    if args.kernel_size != args.dilation_coefficient:\n",
    "        print(\"WARNING: The dilation coefficient is not equal to the kernel size. Multiple paths can lead to the same delays. Set kernel_size equal to dilation_c to have exaxtly one path for each delay.\")\n",
    "\n",
    "    kernel_size = args.kernel_size\n",
    "    levels = args.hidden_layers+1\n",
    "    nrepochs = args.epochs\n",
    "    learningrate = args.learning_rate\n",
    "    optimizername = args.optimizer\n",
    "    dilation_c = args.dilation_coefficient\n",
    "    loginterval = args.log_interval\n",
    "    seed=args.seed\n",
    "    cuda=args.cuda\n",
    "    significance=args.significance\n",
    "\n",
    "    tic = time.time()\n",
    "    if args.ground_truth is not None:\n",
    "        datafiles = args.ground_truth\n",
    "        exp_records = main(datafiles, args, True)\n",
    "\n",
    "    else:\n",
    "        datafiles = dict()\n",
    "        for dataset in args.data:\n",
    "            datafiles[dataset]=\"\"\n",
    "        exp_records = main(datafiles, args, False)\n",
    "    toc = time.time() - tic\n",
    "\n",
    "    import datetime\n",
    "    import os\n",
    "    import pickle\n",
    "    # Use the timezone in my location.\n",
    "    local_tz = datetime.timezone(datetime.timedelta(hours=8))\n",
    "    time_str = datetime.datetime.now(local_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    fname = os.path.join(\"temp_results\", \"tcdf\", \"tcdf_fmri\", f\"exp_records_sign{args.significance:.1f}_{time_str}.pkl\")\n",
    "    print(fname)\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    with open(fname, \"wb\") as f:\n",
    "    #     pickle.dump(exp_rets, f)\n",
    "        pickle.dump({'exp_records': exp_records, 'time': toc}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
